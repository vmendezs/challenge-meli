ğŸ“‚ Â¡Bienvenidos Challenge Datapipeline! ğŸ“‚

AquÃ­, en esta secciÃ³n de mi repositorio, encontrarÃ¡s:

ğŸ Codigo con la soluciÃ³n: Codigo python "build_dataset.py" con la soluciÃ³n del reto.

ğŸ—‚ï¸ Resultado de la soluciÃ³n: Dentro de "dataset.csv", se encuentran los items obtenidos de la api correspondiente.

ğŸ“œ Cabe aclarar unos puntos sobre la soluciÃ³n:
1. El resultado de la soluciÃ³n no cumple con la cantidad requerida de registros (+/- 5mil) dadas las condiciones de la API.
2. Del punto anterior, se debe aclarar que el lÃ­mite de la paginaciÃ³n es de 50 items por cada pÃ¡gina y el valor mÃ¡ximo del offset es 1000; por tal razÃ³n, la cantidad mÃ¡xima de items que puede obtener el datapipeline es de 1000 items aproximadamente. Esto sucede porque el offset debe ser fijado en saltos que le permitan no repetir items por paginaciÃ³n, de forma que, el offset salta 50 items por iteraciÃ³n y al tener un mÃ¡ximo valor 1000, no se puede obtener mÃ¡s informaciÃ³n.
3. A pesar de estas limitaciones, el cÃ³digo esta diseÃ±ado para que funcione con los mismos campos pero limitaciones diferentes. En ese sentido, si la configuraciÃ³n de la api cambiara y el lÃ­mite por paginaciÃ³n fuera mÃ¡s alto, el valor mÃ¡ximo del offset fuera mÃ¡s alto o incluyendo otro tipo de variables como "next_page" dentro de la api, se podrÃ­a evaluar una nueva forma de alcanzar la cantidad de datos requerida.
